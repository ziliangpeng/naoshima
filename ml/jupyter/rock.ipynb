{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'rock_paper_scissors'\n",
    "\n",
    "(dataset_train_raw, dataset_test_raw), dataset_info = tfds.load(\n",
    "    name=DATASET_NAME,\n",
    "    data_dir='tmp',\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EXAMPLES = dataset_info.splits['train'].num_examples\n",
    "NUM_TEST_EXAMPLES = dataset_info.splits['test'].num_examples\n",
    "NUM_CLASSES = dataset_info.features['label'].num_classes\n",
    "\n",
    "print('Number of TRAIN examples:', NUM_TRAIN_EXAMPLES)\n",
    "print('Number of TEST examples:', NUM_TEST_EXAMPLES)\n",
    "print('Number of label classes:', NUM_CLASSES)\n",
    "\n",
    "INPUT_IMG_SIZE_ORIGINAL = dataset_info.features['image'].shape[0]\n",
    "INPUT_IMG_SHAPE_ORIGINAL = dataset_info.features['image'].shape\n",
    "\n",
    "INPUT_IMG_SIZE_REDUCED = INPUT_IMG_SIZE_ORIGINAL // 2\n",
    "INPUT_IMG_SHAPE_REDUCED = (\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_IMG_SIZE_REDUCED,\n",
    "    INPUT_IMG_SHAPE_ORIGINAL[2]\n",
    ")\n",
    "\n",
    "# Here we may switch between bigger or smaller image sized that we will train our model on.\n",
    "INPUT_IMG_SIZE = INPUT_IMG_SIZE_REDUCED\n",
    "INPUT_IMG_SHAPE = INPUT_IMG_SHAPE_REDUCED\n",
    "\n",
    "print('Input image size (original):', INPUT_IMG_SIZE_ORIGINAL)\n",
    "print('Input image shape (original):', INPUT_IMG_SHAPE_ORIGINAL)\n",
    "print('\\n')\n",
    "print('Input image size (reduced):', INPUT_IMG_SIZE_REDUCED)\n",
    "print('Input image shape (reduced):', INPUT_IMG_SHAPE_REDUCED)\n",
    "print('\\n')\n",
    "print('Input image size:', INPUT_IMG_SIZE)\n",
    "print('Input image shape:', INPUT_IMG_SHAPE)\n",
    "\n",
    "get_label_name = dataset_info.features['label'].int2str\n",
    "\n",
    "def preview_dataset(dataset):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_index = 0\n",
    "    for features in dataset.take(12):\n",
    "        (image, label) = features\n",
    "        plot_index += 1\n",
    "        plt.subplot(3, 4, plot_index)\n",
    "        # plt.axis('Off')\n",
    "        label = get_label_name(label.numpy())\n",
    "        plt.title('Label: %s' % label)\n",
    "        plt.imshow(image.numpy())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(image, label):\n",
    "    # Make image color values to be float.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Make image color values to be in [0..1] range.\n",
    "    image = image / 255.\n",
    "    # Make sure that image has a right size\n",
    "    image = tf.image.resize(image, [INPUT_IMG_SIZE, INPUT_IMG_SIZE])\n",
    "    return image, label\n",
    "\n",
    "\n",
    "dataset_train = dataset_train_raw.map(format_example)\n",
    "dataset_test = dataset_test_raw.map(format_example)\n",
    "\n",
    "(first_image, first_lable) = list(dataset_train.take(1))[0]\n",
    "print('Label:', first_lable.numpy(), '\\n')\n",
    "print('Image shape:', first_image.numpy().shape, '\\n')\n",
    "print(first_image.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_flip(image: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    return image\n",
    "\n",
    "def augment_color(image: tf.Tensor) -> tf.Tensor:\n",
    "    image = tf.image.random_hue(image, max_delta=0.08)\n",
    "    image = tf.image.random_saturation(image, lower=0.7, upper=1.3)\n",
    "    image = tf.image.random_brightness(image, 0.05)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1)\n",
    "    image = tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)\n",
    "    return image\n",
    "\n",
    "def augment_rotation(image: tf.Tensor) -> tf.Tensor:\n",
    "    # Rotate 0, 90, 180, 270 degrees\n",
    "    return tf.image.rot90(\n",
    "        image,\n",
    "        tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "    )\n",
    "\n",
    "def augment_inversion(image: tf.Tensor) -> tf.Tensor:\n",
    "    random = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "    if random > 0.5:\n",
    "        image = tf.math.multiply(image, -1)\n",
    "        image = tf.math.add(image, 1)\n",
    "    return image\n",
    "\n",
    "def augment_zoom(image: tf.Tensor, min_zoom=0.8, max_zoom=1.0) -> tf.Tensor:\n",
    "    image_width, image_height, image_colors = image.shape\n",
    "    crop_size = (image_width, image_height)\n",
    "\n",
    "    # Generate crop settings, ranging from a 1% to 20% crop.\n",
    "    scales = list(np.arange(min_zoom, max_zoom, 0.01))\n",
    "    boxes = np.zeros((len(scales), 4))\n",
    "\n",
    "    for i, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - (0.5 * scale)\n",
    "        x2 = y2 = 0.5 + (0.5 * scale)\n",
    "        boxes[i] = [x1, y1, x2, y2]\n",
    "\n",
    "    def random_crop(img):\n",
    "        # Create different crops for an image\n",
    "        crops = tf.image.crop_and_resize(\n",
    "            [img],\n",
    "            boxes=boxes,\n",
    "            box_indices=np.zeros(len(scales)),\n",
    "            crop_size=crop_size\n",
    "        )\n",
    "        # Return a random crop\n",
    "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
    "\n",
    "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "\n",
    "    # Only apply cropping 50% of the time\n",
    "    return tf.cond(choice < 0.5, lambda: image, lambda: random_crop(image))\n",
    "\n",
    "def augment_data(image, label):\n",
    "    image = augment_flip(image)\n",
    "    image = augment_color(image)\n",
    "    image = augment_rotation(image)\n",
    "    image = augment_zoom(image)\n",
    "    image = augment_inversion(image)\n",
    "    return image, label\n",
    "\n",
    "dataset_train_augmented = dataset_train.map(augment_data)\n",
    "\n",
    "preview_dataset(dataset_train_augmented)\n",
    "\n",
    "preview_dataset(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented.shuffle(\n",
    "    buffer_size=NUM_TRAIN_EXAMPLES\n",
    ")\n",
    "\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented.batch(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Prefetch will enable the input pipeline to asynchronously fetch batches while your model is training.\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented_shuffled.prefetch(\n",
    "    buffer_size=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "dataset_test_shuffled = dataset_test.batch(BATCH_SIZE)\n",
    "\n",
    "print(dataset_train_augmented_shuffled)\n",
    "print(dataset_test_shuffled)\n",
    "\n",
    "# Debugging the batches using conversion to Numpy arrays.\n",
    "batches = tfds.as_numpy(dataset_train_augmented_shuffled)\n",
    "for batch in batches:\n",
    "    image_batch, label_batch = batch\n",
    "    print('Label batch shape:', label_batch.shape, '\\n')\n",
    "    print('Image batch shape:', image_batch.shape, '\\n')\n",
    "    print('Label batch:', label_batch, '\\n')\n",
    "    \n",
    "    for batch_item_index in range(len(image_batch)):\n",
    "        print('First batch image:', image_batch[batch_item_index], '\\n')\n",
    "        plt.imshow(image_batch[batch_item_index])\n",
    "        plt.show()\n",
    "        # Break to shorten the output.\n",
    "        break\n",
    "    # Break to shorten the output.\n",
    "    break\n",
    "    \n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# First convolution.\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    input_shape=INPUT_IMG_SHAPE,\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Second convolution.\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Third convolution.\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Fourth convolution.\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Flatten the results to feed into dense layers.\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "# 512 neuron dense layer.\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=512,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "\n",
    "# Output layer.\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=NUM_CLASSES,\n",
    "    activation=tf.keras.activations.softmax\n",
    "))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=rmsprop_optimizer,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // BATCH_SIZE\n",
    "validation_steps = NUM_TEST_EXAMPLES // BATCH_SIZE\n",
    "\n",
    "print('steps_per_epoch:', steps_per_epoch)\n",
    "print('validation_steps:', validation_steps)\n",
    "\n",
    "# Preparing callbacks.\n",
    "os.makedirs('logs/fit', exist_ok=True)\n",
    "tensorboard_log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=tensorboard_log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "os.makedirs('tmp/checkpoints', exist_ok=True)\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='tmp/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    monitor='val_accuracy'\n",
    "    # monitor='val_loss'\n",
    ")\n",
    "\n",
    "training_history = model.fit(\n",
    "    x=dataset_train_augmented_shuffled.repeat(),\n",
    "    validation_data=dataset_test_shuffled.repeat(),\n",
    "    epochs=15,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[\n",
    "        # model_checkpoint_callback,\n",
    "        # early_stopping_callback,\n",
    "        tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
